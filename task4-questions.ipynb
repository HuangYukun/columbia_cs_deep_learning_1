{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040 Assignment 1, Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: They are both supervised learning methods but work differently. Support Vector Machine: basically it's a method to optimize the boundary/hyperplane given by linear classifiers like perceptron. More precisely, to optimize the boundary is to maximize the margin to the closest data points of each set. This is the cost function of SVM.\n",
    "   Neural network: Neural network has input, hidden, and output nodes. Each node takes a function, it can be softmax, linear, logistic or even SVM and returns an output. A weighted average of output from nodes in a layer would be passed to nodes of its proceeding as input until it reaches the output node. It's cost function is to adjust the weights to minimize the error.\n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: It's easier to optimize with SGD and backpropagation than smooth activation functions, such as sigmoid and tanh. \n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: To be honest, my best model is still not reaching 48%. I raised the hidden_dim to 150, by trying from 100-500, a high dimension would not always help. I changed reg to 0.1 instead of 0.25, to make the model learn 'more' about the data, of course, it causes overfitting in the meantime. It's a tradeoff. I raised the learning rate to 0.002. I changed the batch_size to 800. I also tried to play around with a higher learning rate etc, but the problem is even though I implemented softmax as suggested to avoid overflow, it still somehow being not 'stable'. \n",
    "    \n",
    "    model = TwoLayerNet(input_dim=3072, hidden_dim=150, num_classes=10, reg=0.1, weight_scale=1e-3)\n",
    "    num_epoch = 8\n",
    "    batch_size = 800 #\n",
    "    lr = 2e-3 #\n",
    "    verbose = True\n",
    "    ld = 0.95\n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer: Divide the original samples into k subsamples. Use 1 out of the k subsamples as validation set for testing the model, the other k-1 subsamples as training sets. Repeat this procedure for k times to ensure each of the k subsamples are used exactly once as the validation set. We can take the average of the k results to form an estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
